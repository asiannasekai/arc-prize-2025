{
  "overall_accuracy": 0.0,
  "pattern_type_accuracy": {
    "extraction": 0.0,
    "rule": 0.0,
    "tiling": 0.0
  },
  "difficulty_analysis": {
    "medium": {
      "accuracy": 0.0,
      "count": 9,
      "avg_confidence": 0.3525462369098629,
      "avg_processing_time": 0.018395794762505427
    },
    "hard": {
      "accuracy": 0.0,
      "count": 1,
      "avg_confidence": 0.36445442511678294,
      "avg_processing_time": 0.01664447784423828
    }
  },
  "expert_performance": {
    "tiling": {
      "accuracy": 0.0,
      "usage_count": 7,
      "avg_contribution": 0.5244033024307374
    },
    "mapping": {
      "accuracy": 0.0,
      "usage_count": 9,
      "avg_contribution": 0.6496471046453742
    },
    "extraction": {
      "accuracy": 0.0,
      "usage_count": 1,
      "avg_contribution": 0.4823529411764706
    }
  },
  "confidence_calibration": {
    "0.3": {
      "predicted_accuracy": 0.3,
      "actual_accuracy": 0.0,
      "count": 10,
      "calibration_error": 0.3
    }
  },
  "processing_efficiency": {
    "avg_time_per_task": 0.01822066307067871,
    "median_time_per_task": 0.01578664779663086,
    "std_time_per_task": 0.016629118548048116,
    "tasks_per_second": 54.88274472344713
  },
  "error_patterns": [],
  "benchmark_comparison": {
    "arc_agi_baseline": 0.04,
    "human_performance": 1.0,
    "our_performance": 0.0,
    "improvement_over_baseline": 0.0,
    "distance_to_prize_target": 0.85,
    "distance_to_human": 1.0
  },
  "evaluation_metadata": {
    "total_tasks_evaluated": 10,
    "evaluation_timestamp": 1758480147.3724537
  }
}